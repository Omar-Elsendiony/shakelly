{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re   \n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def save_binary(data, file, folder):\n",
    "    location  = os.path.join(folder, (file+'.pickle') )\n",
    "    with open(location, 'wb') as ff:\n",
    "        pickle.dump(data, ff, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_binary(file, folder):\n",
    "    location  = os.path.join(folder, (file+'.pickle') )\n",
    "    with open(location, 'rb') as ff:\n",
    "        data = pickle.load(ff)\n",
    "    return data\n",
    "\n",
    "harakat= load_binary('diacritics','./')\n",
    "\n",
    "def clear_tashkel(text):\n",
    "    text = \"\".join(c for c in text if c not in harakat)\n",
    "    return text\n",
    "\n",
    "def get_tashkel(sentence):\n",
    "    # harakat= load_binary('diacritics','./')\n",
    "    output = []\n",
    "    current_haraka = \"\"\n",
    "    chIndex = 0\n",
    "    mode = 0  # mode 0 is character meant (expecting to get character) and mode 1 is tashkeel ment (expecting to get tashkeel)\n",
    "    while chIndex < (len(sentence)):\n",
    "        characterTashkeels = []\n",
    "        if mode == 1:\n",
    "            while chIndex < (len(sentence)) and sentence[chIndex] in harakat:\n",
    "                characterTashkeels.append(sentence[chIndex])\n",
    "                chIndex += 1\n",
    "\n",
    "            if (len(characterTashkeels) != 0):\n",
    "                output.append(characterTashkeels)\n",
    "            else:\n",
    "                output.append(\"_\") # no tashkeel for now\n",
    "            # chIndex += 1\n",
    "            mode = 0\n",
    "        else:\n",
    "            mode = 1\n",
    "            chIndex += 1\n",
    "    \n",
    "    if mode == 1: # now I am exepcting tashkeel but the word ended before I find one\n",
    "        output.append(\"_\")  # _ symbolizes no tashkeel\n",
    "    return output\n",
    "\n",
    "\n",
    "arabic_alphabet_set=load_binary('arabic_letters','./')\n",
    "arabic_alphabet = dict(zip(arabic_alphabet_set, list(range(len(arabic_alphabet_set)))))\n",
    "\n",
    "def get_char_vector(char):\n",
    "    if char in arabic_alphabet:\n",
    "        vector = [0 for _ in range(len(arabic_alphabet))]\n",
    "        vector[arabic_alphabet[char] ] = 1\n",
    "        return vector\n",
    "    else:\n",
    "        return list(np.ones(len(arabic_alphabet),dtype=int) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "# read the training set\n",
    "def readFile(file_path):\n",
    "    # Open the file for reading\n",
    "    file_content = None\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Read the content of the file\n",
    "        file_content = file.read()\n",
    "    return (file_content)\n",
    "\n",
    "def cleanText(data):\n",
    "    punctuations=[\"،\",\"؛\",\"؟\",\".\",\",\",\"!\",\":\",\" \"]\n",
    "    listTobeChecked = set(tuple(list(harakat)+list(arabic_alphabet.keys())+punctuations))\n",
    "    for char in data:\n",
    "        #check if char is not in arabic alphabet or harakat\n",
    "        if char not in (listTobeChecked):\n",
    "            data = data.replace(char,'')\n",
    "    # split text into sentences using regex\n",
    "    \n",
    "    data = re.sub('  +',' ', data)\n",
    "    \n",
    "    data = re.split(r'[،؛؟.,!:]+', data)\n",
    "    \n",
    "    return data\n",
    "def get_sentences(sentences):\n",
    "    words_per_sentence = []\n",
    "    tashkeel_per_sentence = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence == '': continue # new line may be followed by full stop\n",
    "        sentence = word_tokenize(sentence)\n",
    "        words_per_sentence.append(sentence)\n",
    "    \n",
    "    tashkeel_per_sentence = [[None for _ in w ]for w in words_per_sentence]\n",
    "    for i, x in enumerate(words_per_sentence):\n",
    "        for j, y in enumerate(x):\n",
    "            tashkeel = get_tashkel(y)\n",
    "            words_per_sentence[i][j] = clear_tashkel(y)\n",
    "            tashkeel_per_sentence[i][j] = tashkeel\n",
    "    return words_per_sentence, tashkeel_per_sentence\n",
    "\n",
    "# sent=[\"عَُمرً\"] \n",
    "\n",
    "# %%\n",
    "from gensim.models import  FastText\n",
    "# Define and train Word2Vec model\n",
    "def makeModel(dataset):\n",
    "    model = FastText(sentences=dataset, vector_size=100, window=15, min_count=1, workers=40)\n",
    "    # Save the trained model (optional)\n",
    "    model.save(\"model.model\")\n",
    "    return model\n",
    "\n",
    "def getCharacterEncoding(word):\n",
    "    word_embedding = list()\n",
    "    for w in word:\n",
    "        word_embedding.append(get_char_vector(w))\n",
    "    return word_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DELL\\anaconda3\\envs\\image_env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 100\n",
    "embedding_dim = 100\n",
    "max_char_length = 15\n",
    "char_vocab_size = 36\n",
    "num_diacritics = 15\n",
    "#very small number\n",
    "lstm_units = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=load_binary('sentences_without_diacritics','cleaned_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embedding =  FastText.load(\"model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def writeToCSV(data):\n",
    "    with open('submission.csv', mode='w', newline='') as csv_file:  # Added newline=''\n",
    "        fieldnames = ['ID', 'label']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i in range(len(data)):\n",
    "            writer.writerow({'ID': i, 'label': data[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13043/13043 [==============================] - 200s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from preprocessing import *\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "max_sentence_length = 30\n",
    "embedding_dim = 100\n",
    "max_char_length = 15\n",
    "char_vocab_size = 36\n",
    "num_diacritics = 15\n",
    "model1h5=load_model('model11pm.h5')\n",
    "\n",
    "\n",
    "def split_lists(input_list, max_length):\n",
    "    result_lists = []\n",
    "    for inner_list in input_list:\n",
    "        for i in range(0, len(inner_list), max_length):\n",
    "            result_lists.append(inner_list[i:i + max_length])\n",
    "    return result_lists\n",
    "\n",
    "\n",
    "def test_sent(path):\n",
    "    testset = readFile(path)\n",
    "    sent=cleanText(testset)\n",
    "    \n",
    "    sent, _ = get_sentences(sent)\n",
    "    sent=split_lists(sent,max_sentence_length)\n",
    "\n",
    "    embeddding_model=FastText.load('model.model')\n",
    "\n",
    "    def getEmbeddings(sentences, word2vecmodel):\n",
    "        embeddingSentences = [] # list of all sentences\n",
    "        keys = word2vecmodel.wv.key_to_index\n",
    "        for s in sentences:\n",
    "            embeddingTemp = []  # list for one sentence\n",
    "            for w in s:\n",
    "                embeddingTemp.append(word2vecmodel.wv[w])\n",
    "            embeddingSentences.append(embeddingTemp)\n",
    "        return embeddingSentences\n",
    "\n",
    "    embeddingsSentences = getEmbeddings(sent, embeddding_model)\n",
    "\n",
    "    X_sentence = []  # List for sentence_input\n",
    "    X_char = []      # List for char_input\n",
    "    X_scalar = []    # List for scalar_input\n",
    "\n",
    "    X_chars=[]\n",
    "    for i in range(len(embeddingsSentences)):\n",
    "        for j in range(len(embeddingsSentences[i])):\n",
    "            for char in sent[i][j]:\n",
    "                X_sentence.append(embeddingsSentences[i])\n",
    "                X_char.append(get_char_vector(char))\n",
    "                X_chars.append(getCharacterEncoding(sent[i][j]))\n",
    "                X_scalar.append([j])\n",
    "    X_character_padded = pad_sequences(X_chars, maxlen=max_char_length, padding='post', dtype='float16')\n",
    "    X_chars2 = np.array(X_character_padded)\n",
    "    X_sentence_padded = pad_sequences(X_sentence, maxlen=max_sentence_length, padding='post', dtype='float16')\n",
    "    # Convert lists to numpy arrays\n",
    "    X_sentence = np.array(X_sentence_padded)\n",
    "    X_char = np.array(X_char)\n",
    "    X_scalar = np.array(X_scalar)\n",
    "\n",
    "    return X_sentence, X_char, X_scalar,X_chars2\n",
    "X_sentence, X_char, X_scalar,X_chars2=test_sent(\"Delivery/sample_testset_with_submission/test_no_diacritics.txt\")\n",
    "predictions=model1h5.predict([X_sentence,X_char,X_chars2,X_scalar])\n",
    "pred=np.argmax(predictions, axis=-1)\n",
    "\n",
    "writeToCSV(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
