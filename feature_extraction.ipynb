{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This module purpose is to extract the features of the words and the characters (if we want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfIdf feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (23, 0)\t0.17110975535227513\n",
      "  (33, 0)\t0.49109937786119445\n",
      "  (42, 0)\t0.425968963669734\n",
      "  (230, 0)\t0.5227602070880277\n",
      "  (396, 0)\t0.12354902936131466\n",
      "  (463, 0)\t0.38314850882398366\n",
      "  (464, 0)\t0.3368708615371434\n",
      "  (509, 0)\t0.30727454671293175\n",
      "  (540, 0)\t0.450378651498764\n",
      "  (615, 0)\t0.41573936940388145\n",
      "  (631, 0)\t0.21902042583447204\n",
      "  (920, 0)\t0.18902251973927106\n",
      "  (972, 0)\t0.28024623800137033\n",
      "  (974, 0)\t0.1440563768332292\n",
      "  (1059, 0)\t0.5034907435856646\n",
      "  (1211, 0)\t0.11261416426695998\n",
      "  (1263, 0)\t0.40464437512909984\n",
      "  (1270, 0)\t0.3888170882844598\n",
      "  (1278, 0)\t0.5303671128420577\n",
      "  (1305, 0)\t0.1555589652997581\n",
      "  (1335, 0)\t0.4663460760455351\n",
      "  (1428, 0)\t0.5093133864772488\n",
      "  (1439, 0)\t0.14538408323448207\n",
      "  (1459, 0)\t0.22040734840260992\n",
      "  (1466, 0)\t0.13252452617328214\n",
      "  :\t:\n",
      "  (38098, 0)\t0.2139081027714699\n",
      "  (38373, 0)\t0.1640711960057835\n",
      "  (38504, 0)\t0.2861567011896214\n",
      "  (38516, 0)\t0.1126706047099919\n",
      "  (38560, 0)\t0.25039828467157693\n",
      "  (38585, 0)\t0.20197487602873965\n",
      "  (38746, 0)\t0.32183639148157595\n",
      "  (38795, 0)\t0.3273249804544456\n",
      "  (38796, 0)\t0.20443917328407332\n",
      "  (39000, 0)\t0.07441145920149808\n",
      "  (39038, 0)\t0.09424903615840766\n",
      "  (39079, 0)\t0.26099097640403685\n",
      "  (39108, 0)\t0.3568829529134357\n",
      "  (39189, 0)\t0.054378729325383734\n",
      "  (39208, 0)\t0.156053565968275\n",
      "  (39644, 0)\t0.18658038894964607\n",
      "  (39661, 0)\t0.34132194029109153\n",
      "  (39713, 0)\t0.13144749631907746\n",
      "  (39721, 0)\t0.21940136195713741\n",
      "  (39738, 0)\t0.44638490732917274\n",
      "  (39953, 0)\t0.2657406379726496\n",
      "  (39968, 0)\t0.3904290151507905\n",
      "  (39973, 0)\t0.499802865939634\n",
      "  (40123, 0)\t0.12520402996827473\n",
      "  (40231, 0)\t0.4136861369403293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"dataset/train.txt\"\n",
    "# Open the file for reading\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    # Read the content of the file\n",
    "    file_content = file.read()\n",
    "\n",
    "# Separate based on the delimiter '.'\n",
    "sentences = file_content.split('.')\n",
    "\n",
    "# Remove empty strings from the list (resulting from consecutive '.' characters)\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "# what is tfidf vectorizerr\n",
    "# it is a combination between tfidf CountVectorizer that calculated the count\n",
    "# and tfidf transform that normalizes the results\n",
    "vectorizer = TfidfVectorizer(encoding='utf-8')\n",
    "\n",
    "# what is fit and transform?\n",
    "# fit extracts the count of the instances of this word in this document\n",
    "# then the vector (column or row) includes then th\n",
    "# transform nomalizes the data so that the data values are between 0 and 1\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "tfidf_matrix_dense = tfidf_matrix.todense()\n",
    "# Get the feature names (terms) so that we can search for this word vector (word or term consecu)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# search word\n",
    "search_word = \"10\"\n",
    "if search_word in feature_names:\n",
    "    word_index = list(feature_names).index(search_word)\n",
    "    print(tfidf_matrix[:, word_index])  # the first dim is the documents , the second is the word or term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Define and train Word2Vec model\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model (optional)\n",
    "word2vec_model.save(\"word2vec_model.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the training set\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the matrix to a dense array and display the result\n",
    "dense_array = X.toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
