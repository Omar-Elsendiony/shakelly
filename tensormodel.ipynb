{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 21:43:38.832446: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-31 21:43:38.894944: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-31 21:43:38.895004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-31 21:43:38.896985: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-31 21:43:38.906283: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-31 21:43:38.906982: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-31 21:43:39.953418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 100\n",
    "embedding_dim = 100\n",
    "max_char_length = 15\n",
    "char_vocab_size = 36\n",
    "num_diacritics = 15\n",
    "lstm_units = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([['عَُمرً']], [[['_', '_', '_', '_', '_', '_']]])\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "trainSet = readFile('dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17073/833179504.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(target_backend='cuda')\n",
      "/tmp/ipykernel_17073/833179504.py:5: NumbaWarning: \u001b[1m\n",
      "Compilation is falling back to object mode WITH looplifting enabled because Function \"clean\" failed type inference due to: \u001b[1mUntyped global name 'get_sentences':\u001b[0m \u001b[1m\u001b[1mCannot determine Numba type of <class 'function'>\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../tmp/ipykernel_17073/833179504.py\", line 9:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\u001b[0m\n",
      "  @jit(target_backend='cuda')\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning: \u001b[1mFunction \"clean\" was compiled in object mode without forceobj=True.\n",
      "\u001b[1m\n",
      "File \"../../tmp/ipykernel_17073/833179504.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaWarning(warn_msg,\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning: \u001b[1m\n",
      "Fall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n",
      "\n",
      "For more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n",
      "\u001b[1m\n",
      "File \"../../tmp/ipykernel_17073/833179504.py\", line 5:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaDeprecationWarning(msg,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# from numba import jit, cuda \n",
    "# @jit(target_backend='cuda')\n",
    "\n",
    "\n",
    "def clean():\n",
    "    sentences_without_diacritics, diacritics = get_sentences(cleanText(trainSet[9000000:]))\n",
    "    save_binary(sentences_without_diacritics, 'sentences_without_diacritics_2', './cleaned_data')\n",
    "    save_binary(diacritics, 'diacritics_2', './cleaned_data')\n",
    "\n",
    "clean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# def merge_pickles(file1_path, file2_path, output_path):\n",
    "#     with open(file1_path, 'rb') as f1, open(file2_path, 'rb') as f2, open(output_path, 'wb') as output_file:\n",
    "#         shutil.copyfileobj(f1, output_file)\n",
    "#         shutil.copyfileobj(f2, output_file)\n",
    "\n",
    "# # Example usage\n",
    "# #merge_pickles('cleaned_data/diacritics_1.pickle', 'cleaned_data/diacritics_2.pickle', 'cleaned_data/diacritics.pickle')\n",
    "# merge_pickles('cleaned_data/sentences_without_diacritics_1.pickle', 'cleaned_data/sentences_without_diacritics_2.pickle', 'cleaned_data/sentences_without_diacritics.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['َ'], ['َ'], '_'], [['َ'], ['ُ'], '_', ['ُ']], [['َ'], ['ْ']], [['ُ'], ['ْ'], ['َ'], '_'], [['َ'], '_', ['ِ'], ['َ'], ['ً']], [['ِ'], ['ْ']], [['َ'], ['ْ'], ['ِ']], [['َ'], ['َ'], ['ِ'], ['ِ']], [['ِ'], '_'], ['_', '_', ['ّ', 'ُ'], '_', ['َ'], ['َ'], ['ْ'], ['ِ']], [['َ'], ['ِ'], ['ْ']], [['َ'], ['َ'], '_', ['َ'], ['َ'], '_']]\n"
     ]
    }
   ],
   "source": [
    "# from utils import * \n",
    "\n",
    "# print(load_binary('diacritics','./cleaned_data/')[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sentences_without_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecmodel = makeWord2VecModel(sentences_without_diacritics)\n",
    "keys = word2vecmodel.wv.key_to_index\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingsSentences(sentences, word2vecmodel):\n",
    "    embeddingSentences = [] # list of all sentences\n",
    "    keys = word2vecmodel.wv.key_to_index\n",
    "    for s in sentences:\n",
    "        embeddingTemp = []  # list for one sentence\n",
    "        for w in s:\n",
    "            if w in keys:\n",
    "                embeddingTemp.append(word2vecmodel.wv[w])\n",
    "            ### unknown OOV till now\n",
    "        embeddingSentences.append(embeddingTemp)\n",
    "    return embeddingSentences\n",
    "embeddingsSentences = getEmbeddingsSentences(sentences_without_diacritics, word2vecmodel)\n",
    "model.save(\"word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harakatID   = load_binary('diacritic2id','./')\n",
    "\n",
    "def get_diacritic_hot_vector(haraka):\n",
    "    if haraka not in harakatID:\n",
    "        return list(np.ones(15,dtype=int) )\n",
    "    vector = [0 for _ in range(len(harakatID))]\n",
    "    # print(\"haraka:\" + haraka)\n",
    "    vector[harakatID[haraka] ] = 1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# harakat   = load_binary('diacritic2id','./')\n",
    "# for i in harakat:\n",
    "#     print(i,harakat[i])\n",
    "#يُسَنُّ أَنْ يُصَانَ عَنْ رَائِحَةٍ كَرِيهَةٍ مِنْ بَصَلٍ وَثُومٍ وَكُرَّاتٍ( 3 / 297 )\n",
    "\n",
    "corpusDiacList = []\n",
    "for sentence in diacritics:\n",
    "    sentenceDiacList = []\n",
    "    for word in sentence:\n",
    "        #merge each list \n",
    "        oneDiacStr = ''\n",
    "        diacWordList = []\n",
    "        for diac in word:\n",
    "            if diac == '_':\n",
    "                oneDiacStr = ''\n",
    "            else:    \n",
    "                oneDiacStr = ''.join(diac)\n",
    "            #print(oneDiacStr,harakat[oneDiacStr]) \n",
    "            diacWordList.append(get_diacritic_hot_vector(oneDiacStr)) \n",
    "        sentenceDiacList.append(diacWordList)       \n",
    "    corpusDiacList.append(sentenceDiacList)\n",
    "print(corpusDiacList)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_sentence = []  # List for sentence_input\n",
    "X_char = []      # List for char_input\n",
    "X_scalar = []    # List for scalar_input\n",
    "flattened = [item for sublist in corpusDiacList for item in sublist]\n",
    "Y__padded = pad_sequences(flattened, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "          # List for output\n",
    "\n",
    "for i in range(len(embeddingsSentences)):\n",
    "    for j in range(len(embeddingsSentences[i])):\n",
    "        # Add embeddings\n",
    "        X_sentence.append(embeddingsSentences[i])\n",
    "\n",
    "        # Prepare char_input\n",
    "        char_encoding = getCharacterEncoding(sentences_without_diacritics[i][j])\n",
    "        # Ensure char_encoding is shaped as (max_char_length, char_vocab_size)\n",
    "        # This might require reshaping or padding depending on your getCharacterEncoding function\n",
    "        X_char.append(char_encoding)\n",
    "\n",
    "        # Scalar input\n",
    "        X_scalar.append([j])\n",
    "\n",
    "        # Output\n",
    "        # y = getDiacriticEncoding(diacritics[i][j])\n",
    "        # Y.append(y)\n",
    "\n",
    "# Padding X_sentence\n",
    "# Assuming max_sentence_length and embedding_dim are defined\n",
    "X_sentence_padded = pad_sequences(X_sentence, maxlen=max_sentence_length, padding='post', dtype='float32')\n",
    "X_character_padded = pad_sequences(X_char, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "# Y__padded = pad_sequences(Y, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_sentence = np.array(X_sentence_padded)\n",
    "X_char = np.array(X_character_padded)\n",
    "X_scalar = np.array(X_scalar)\n",
    "\n",
    "Y = np.array(Y__padded)\n",
    "\n",
    "print(X_sentence.shape, X_char.shape, X_scalar.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectHiddenState(Layer):\n",
    "    def _init_(self, **kwargs):\n",
    "        super(SelectHiddenState, self)._init_(**kwargs)\n",
    "\n",
    "    def call(self, lstm_output, scalar_input):\n",
    "        timestep_index = tf.cast(tf.squeeze(scalar_input, axis=-1), tf.int32)\n",
    "        selected_state = tf.gather(lstm_output, timestep_index, batch_dims=1, axis=1)\n",
    "        return selected_state\n",
    "\n",
    "# Parameters\n",
    "max_sentence_length = 100\n",
    "embedding_dim = 100\n",
    "max_char_length = 15\n",
    "char_vocab_size = 36\n",
    "num_diacritics = 15\n",
    "lstm_units = 32\n",
    "\n",
    "# Inputs\n",
    "char_input = Input(shape=(max_char_length, char_vocab_size))\n",
    "sentence_input = Input(shape=(max_sentence_length, embedding_dim))\n",
    "scalar_input = Input(shape=(1,), name='scalar_input')\n",
    "\n",
    "# Padding layer for sentence_input (adjust padding as needed)\n",
    "# sentence_padding_layer = ZeroPadding1D(padding=(1, 1))  # Example padding\n",
    "# padded_sentence_input = sentence_padding_layer(sentence_input)\n",
    "# padding_layer = ZeroPadding1D(padding=(1, 1))  # Example padding\n",
    "# padded_char_input = padding_layer(char_input)\n",
    "# BiLSTM layer\n",
    "bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "bi_lstm_output, forward_h, forward_c, backward_h, backward_c = bi_lstm(sentence_input)\n",
    "print(forward_h)\n",
    "# Select state layer\n",
    "select_state_layer = SelectHiddenState()\n",
    "hidden_state_nth_timestep = select_state_layer(bi_lstm_output, scalar_input)\n",
    "\n",
    "# RNN layer\n",
    "rnn_cell = SimpleRNNCell(32)\n",
    "rnn_layer = RNN(rnn_cell, return_sequences=True)\n",
    "rnn_output = rnn_layer(char_input, initial_state=(forward_c+backward_c)/2)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(num_diacritics, activation='softmax')(rnn_output)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[sentence_input, char_input, scalar_input], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_sentence,X_char,X_scalar], Y, epochs= 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
