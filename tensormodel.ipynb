{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Input,Flatten, Embedding, Bidirectional, LSTM, Dense, Concatenate, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer\n",
    "class SelectHiddenState(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelectHiddenState, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, lstm_output, scalar_input):\n",
    "        # Ensure scalar_input is an integer for indexing\n",
    "        timestep_index = tf.cast(tf.squeeze(scalar_input, axis=-1), tf.int32)\n",
    "        # Gather the specific hidden state for each batch\n",
    "        selected_state = tf.gather(lstm_output, timestep_index, batch_dims=1, axis=1)\n",
    "        return selected_state\n",
    "\n",
    "# Example usage with your model\n",
    "# lstm_output is from your BiLSTM layer\n",
    "# scalar_input is your additional input\n",
    "max_sentence_length = 100  # Maximum length of sentence embeddings\n",
    "embedding_dim = 512        # Dimension of sentence embeddings\n",
    "max_char_length = 15       # Maximum length of a word in characters\n",
    "char_vocab_size = 28       # Number of unique characters\n",
    "num_diacritics = 8         # Number of possible diacritics for each character, including no diacritic\n",
    "\n",
    "# Parameters\n",
    "lstm_units = 32\n",
    "\n",
    "# Character input\n",
    "char_input = Input(shape=(max_char_length, char_vocab_size))\n",
    "\n",
    "# Inputs\n",
    "sentence_input = Input(shape=(max_sentence_length, embedding_dim))\n",
    "scalar_input = Input(shape=(1,), name='scalar_input')\n",
    "\n",
    "# BiLSTM layer with return_state\n",
    "bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "bi_lstm_output, forward_h, forward_c, backward_h, backward_c = bi_lstm(sentence_input)\n",
    "print( forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
    "#, forward_h, forward_c, backward_h, backward_c\n",
    "# Average the forward and backward states (or choose another method to combine them)\n",
    "select_state_layer = SelectHiddenState()\n",
    "hidden_state_nth_timestep = select_state_layer(bi_lstm_output, scalar_input)\n",
    "\n",
    "#hidden_state_nth_timestep = bi_lstm_output[:, scalar_input[1], :]\n",
    "print(bi_lstm_output.shape)\n",
    "\n",
    "# RNN layer with initial state from BiLSTM\n",
    "rnn_cell = SimpleRNNCell(64)\n",
    "rnn_layer = RNN(rnn_cell, return_sequences=True)\n",
    "rnn_output = rnn_layer(char_input,initial_state=hidden_state_nth_timestep)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(num_diacritics, activation='softmax')(rnn_output)\n",
    "\n",
    "# Build and compile the model\n",
    "# Assuming sentence_input and scalar_input are defined as Input layers\n",
    "model = Model(inputs=[sentence_input, char_input,scalar_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
