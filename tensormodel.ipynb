{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32) (None, 32) (None, 32) (None, 32)\n",
      "(None, 100, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 100, 100)]   0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  [(None, 100, 64),    34048       ['input_2[0][0]']                \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32)]                                                      \n",
      "                                                                                                  \n",
      " scalar_input (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 15, 37)]     0           []                               \n",
      "                                                                                                  \n",
      " select_hidden_state (SelectHid  (None, 64)          0           ['bidirectional[0][0]',          \n",
      " denState)                                                        'scalar_input[0][0]']           \n",
      "                                                                                                  \n",
      " rnn (RNN)                      (None, 15, 64)       6528        ['input_1[0][0]',                \n",
      "                                                                  'select_hidden_state[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 15, 9)        585         ['rnn[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,161\n",
      "Trainable params: 41,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer\n",
    "class SelectHiddenState(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SelectHiddenState, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, lstm_output, scalar_input):\n",
    "        # Ensure scalar_input is an integer for indexing\n",
    "        timestep_index = tf.cast(tf.squeeze(scalar_input, axis=-1), tf.int32)\n",
    "        # Gather the specific hidden state for each batch\n",
    "        selected_state = tf.gather(lstm_output, timestep_index, batch_dims=1, axis=1)\n",
    "        return selected_state\n",
    "\n",
    "# Example usage with your model\n",
    "# lstm_output is from your BiLSTM layer\n",
    "# scalar_input is your additional input\n",
    "max_sentence_length = 100  # Maximum length of sentence embeddings\n",
    "embedding_dim = 100        # Dimension of sentence embeddings\n",
    "max_char_length = 15       # Maximum length of a word in characters\n",
    "char_vocab_size = 37       # Number of unique characters\n",
    "num_diacritics = 9         # Number of possible diacritics for each character, including no diacritic\n",
    "\n",
    "# Parameters\n",
    "lstm_units = 32\n",
    "\n",
    "# Character input\n",
    "char_input = Input(shape=(max_char_length, char_vocab_size))\n",
    "\n",
    "# Inputs\n",
    "sentence_input = Input(shape=(max_sentence_length, embedding_dim))\n",
    "scalar_input = Input(shape=(1,), name='scalar_input')\n",
    "\n",
    "# BiLSTM layer with return_state\n",
    "bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "bi_lstm_output, forward_h, forward_c, backward_h, backward_c = bi_lstm(sentence_input)\n",
    "print( forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
    "#, forward_h, forward_c, backward_h, backward_c\n",
    "# Average the forward and backward states (or choose another method to combine them)\n",
    "select_state_layer = SelectHiddenState()\n",
    "hidden_state_nth_timestep = select_state_layer(bi_lstm_output, scalar_input)\n",
    "\n",
    "#hidden_state_nth_timestep = bi_lstm_output[:, scalar_input[1], :]\n",
    "print(bi_lstm_output.shape)\n",
    "\n",
    "# RNN layer with initial state from BiLSTM\n",
    "rnn_cell = SimpleRNNCell(64)\n",
    "rnn_layer = RNN(rnn_cell, return_sequences=True)\n",
    "rnn_output = rnn_layer(char_input,initial_state=hidden_state_nth_timestep)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(num_diacritics, activation='softmax')(rnn_output)\n",
    "\n",
    "# Build and compile the model\n",
    "# Assuming sentence_input and scalar_input are defined as Input layers\n",
    "model = Model(inputs=[sentence_input, char_input,scalar_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \n",
    "# Y = \n",
    "from preprocessing import *\n",
    "\n",
    "trainSet = readFile('dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_without_diacritics, diacritics = get_sentences(trainSet[400:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ريح', 'النجاسة', 'والسجود', 'للصنم', 'ونحو', 'ذلك', 'وسحر', 'محمد', 'قول', 'مالك', 'وأصحابه', 'أن', 'الساحر', 'كافر', 'بالله', 'تعالى', 'قال', 'مالك', 'هو', 'كالزنديق', 'إذا', 'عمل', 'السحر', 'بنفسه', 'قتل', 'ولم', 'يستتب'], ['قوله', 'لعدم', 'ما', 'تتعلق', 'إلخ', 'أي', 'الوصية', 'قوله', 'ما', 'مر', 'أي', 'قبيل', 'قول', 'المتن', 'لغت', 'ولو', 'اقتصر', 'على', 'أوصيت', 'له', 'بشاة', 'أو', 'أعطوه', 'شاة', 'ولا', 'غنم', 'له', 'عند', 'الموت', 'هل', 'تبطل', 'الوصية', 'أو', 'يشترى', 'له', 'شاة', 'ويؤخذ', 'من', 'قوله', 'الآتي', 'كما', 'لو', 'لم', 'يقل', 'من', 'مالي', 'ولا', 'من', 'غنمي', 'أنها', 'لا', 'تبطل'], ['وعبارة', 'الكنز', 'ولو', 'لم', 'يقل', 'من', 'مالي', 'ولا', 'من', 'غنمي', 'لم', 'يتعين', 'غنمه', 'إن', 'كانت', 'انتهت', 'ا', 'ه', 'سم', 'قوله', 'فيعطى', 'واحدة', 'منها', 'إلخ', 'كما', 'لو', 'كانت', 'موجودة', 'عند', 'الوصية', 'والموت'], ['ولا', 'يجوز', 'أن', 'يعطى', 'واحدة', 'من', 'غير', 'غنمه', 'في', 'الصورتين', 'وإن', 'تراضيا'], ['لأنه', 'صلح', 'على', 'مجهول', 'مغني', 'ونهاية', 'قال', 'ع', 'ش', 'قوله', 'واحدة', 'منها', 'أي', 'كاملة'], ['ولا', 'يجوز']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_without_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['ِ', '_', 'ِ'], ['_', '_', 'َّ', 'َ', '_', 'َ', 'ِ'], ['َ', '_', '_', 'ُّ', 'ُ', '_', 'ِ'], ['ِ', '_', 'َّ', 'َ', 'ِ'], ['َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'َ'], ['َ', 'ِ', 'ْ', 'ٍ'], ['ُ', 'َ', 'َّ', 'ٌ'], ['َ', 'ْ', 'ُ'], ['َ', '_', 'ِ', 'ٍ'], ['َ', 'َ', 'ْ', 'َ', '_', 'ِ', 'ِ'], ['َ', 'َّ'], ['_', '_', 'َّ', '_', 'ِ', 'َ'], ['َ', '_', 'ِ', 'ٌ'], ['ِ', 'َ', '_', 'َّ', 'ِ'], ['َ', 'َ', '_', 'َ', '_'], ['َ', '_', 'َ'], ['َ', '_', 'ِ', 'ٌ'], ['ُ', 'َ'], ['َ', '_', '_', 'ِّ', 'ْ', 'ِ', '_', 'ِ'], ['_', 'َ', '_'], ['َ', 'ِ', 'َ'], ['_', '_', 'ِّ', 'ْ', 'َ'], ['ِ', 'َ', 'ْ', 'ِ', 'ِ'], ['ُ', 'ِ', 'َ'], ['َ', 'َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', 'ْ']], [['َ', 'ْ', 'ُ', 'ُ'], ['ِ', 'َ', 'َ', 'ِ'], ['َ', '_'], ['َ', 'َ', 'َ', 'َّ', 'ُ'], ['_', 'َ', 'ْ'], ['َ', 'ْ'], ['_', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', '_'], ['َ', 'َّ'], ['َ', 'ْ'], ['ُ', 'َ', 'ْ', 'َ'], ['َ', 'ْ', 'ِ'], ['_', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'َ', 'ْ'], ['_', 'ْ', 'َ', 'َ', 'َ'], ['َ', 'َ', '_'], ['َ', 'ْ', 'َ', 'ْ', '_'], ['َ', 'ُ'], ['ِ', 'َ', '_', 'ٍ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', '_', 'ُ'], ['َ', '_', 'ً'], ['َ', 'َ', '_'], ['َ', 'َ', 'َ'], ['َ', 'ُ'], ['ِ', 'ْ', 'َ'], ['_', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'ْ'], ['َ', 'ْ', 'ُ', 'ُ'], ['_', 'ْ', 'َ', 'ِ', 'َّ', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', 'َ', '_'], ['َ', 'ُ'], ['َ', '_', 'ٌ'], ['َ', 'ُ', 'ْ', 'َ', 'ُ'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ', 'ِ'], ['_', 'ْ', '_', 'ِ', '_'], ['َ', 'َ', '_'], ['َ', 'ْ'], ['َ', 'ْ'], ['َ', 'ُ', 'ْ'], ['ِ', 'ْ'], ['َ', '_', 'ِ', '_'], ['َ', 'َ', '_'], ['ِ', 'ْ'], ['َ', 'َ', 'ِ', '_'], ['َ', 'َّ', 'َ', '_'], ['َ', '_'], ['َ', 'ْ', 'ُ', 'ُ']], [['َ', 'ِ', 'َ', '_', 'َ', 'ُ'], ['_', 'ْ', 'َ', 'ْ', 'ِ'], ['َ', 'َ', 'ْ'], ['َ', 'ْ'], ['َ', 'ُ', 'ْ'], ['ِ', 'ْ'], ['َ', '_', 'ِ', '_'], ['َ', 'َ', '_'], ['ِ', 'ْ'], ['َ', 'َ', 'ِ', '_'], ['َ', 'ْ'], ['َ', 'َ', 'َ', 'َّ', 'ْ'], ['َ', 'َ', 'ُ', 'ُ'], ['_', 'ْ'], ['َ', '_', 'َ', 'ْ'], ['_', 'ْ', 'َ', 'َ', 'ْ'], ['_'], ['_'], ['_', '_'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', 'ُ', 'ْ', 'َ', '_'], ['َ', '_', 'ِ', 'َ', 'ً'], ['ِ', 'ْ', 'َ', '_'], ['_', 'َ', 'ْ'], ['َ', 'َ', '_'], ['َ', 'ْ'], ['َ', '_', 'َ', 'ْ'], ['َ', 'ْ', 'ُ', '_', 'َ', 'ً'], ['ِ', 'ْ', 'َ'], ['_', 'ْ', 'َ', 'ِ', 'َّ', 'ِ'], ['َ', '_', 'ْ', 'َ', 'ْ', 'ِ']], [['َ', 'َ', '_'], ['َ', 'ُ', '_', 'ُ'], ['َ', 'ْ'], ['ُ', 'ْ', 'َ', '_'], ['َ', '_', 'ِ', 'َ', 'ً'], ['ِ', 'ْ'], ['َ', 'ْ', 'ِ'], ['َ', 'َ', 'ِ', 'ِ'], ['ِ', '_'], ['_', '_', 'ُّ', '_', 'َ', 'َ', 'ْ', 'ِ'], ['َ', 'ِ', 'ْ'], ['َ', 'َ', '_', 'َ', 'َ', '_']], [['ِ', 'َ', 'َّ', 'ُ'], ['ُ', 'ْ', 'ٌ'], ['َ', 'َ', '_'], ['َ', 'ْ', 'ُ', '_', 'ٍ'], ['ُ', 'ْ', 'ِ', '_'], ['َ', 'ِ', 'َ', '_', 'َ', 'ٌ'], ['َ', '_', 'َ'], ['_'], ['_'], ['َ', 'ْ', 'ُ', 'ُ'], ['َ', '_', 'ِ', 'َ', 'ً'], ['ِ', 'ْ', 'َ', '_'], ['َ', 'ْ'], ['َ', '_', 'ِ', 'َ', 'ً']], [['َ', 'َ', '_'], ['َ', 'ُ', '_', 'ُ']]]\n"
     ]
    }
   ],
   "source": [
    "print(diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'من': 0, 'ولا': 1, 'قوله': 2, 'لم': 3, 'له': 4, 'واحدة': 5, 'الوصية': 6, 'أي': 7, 'لو': 8, 'كما': 9, 'ولو': 10, 'غنمه': 11, 'غنمي': 12, 'كانت': 13, 'ما': 14, 'تبطل': 15, 'قال': 16, 'منها': 17, 'يقل': 18, 'إلخ': 19, 'عند': 20, 'يجوز': 21, 'أن': 22, 'مالك': 23, 'قول': 24, 'شاة': 25, 'أو': 26, 'على': 27, 'مالي': 28, 'تتعلق': 29, 'لعدم': 30, 'مر': 31, 'قبيل': 32, 'المتن': 33, 'بنفسه': 34, 'يستتب': 35, 'الساحر': 36, 'النجاسة': 37, 'والسجود': 38, 'للصنم': 39, 'ونحو': 40, 'ذلك': 41, 'وسحر': 42, 'محمد': 43, 'وأصحابه': 44, 'كافر': 45, 'ولم': 46, 'بالله': 47, 'تعالى': 48, 'هو': 49, 'كالزنديق': 50, 'إذا': 51, 'عمل': 52, 'السحر': 53, 'قتل': 54, 'لغت': 55, 'كاملة': 56, 'اقتصر': 57, 'ه': 58, 'فيعطى': 59, 'موجودة': 60, 'والموت': 61, 'يعطى': 62, 'غير': 63, 'في': 64, 'الصورتين': 65, 'وإن': 66, 'تراضيا': 67, 'لأنه': 68, 'صلح': 69, 'مجهول': 70, 'مغني': 71, 'ونهاية': 72, 'ع': 73, 'سم': 74, 'ا': 75, 'أوصيت': 76, 'انتهت': 77, 'بشاة': 78, 'أعطوه': 79, 'ش': 80, 'غنم': 81, 'الموت': 82, 'هل': 83, 'يشترى': 84, 'ويؤخذ': 85, 'الآتي': 86, 'أنها': 87, 'لا': 88, 'وعبارة': 89, 'الكنز': 90, 'يتعين': 91, 'إن': 92, 'ريح': 93}\n"
     ]
    }
   ],
   "source": [
    "word2vecmodel = makeWord2VecModel(sentences_without_diacritics)\n",
    "keys = word2vecmodel.wv.key_to_index\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingsSentences(sentences, word2vecmodel):\n",
    "    embeddingSentences = [] # list of all sentences\n",
    "    keys = word2vecmodel.wv.key_to_index\n",
    "    for s in sentences:\n",
    "        embeddingTemp = []  # list for one sentence\n",
    "        for w in s:\n",
    "            if w in keys:\n",
    "                embeddingTemp.append(word2vecmodel.wv[w])\n",
    "            ### unknown OOV till now\n",
    "        embeddingSentences.append(embeddingTemp)\n",
    "    return embeddingSentences\n",
    "embeddingsSentences = getEmbeddingsSentences(sentences_without_diacritics, word2vecmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "harakat   = {1614:1,1615:2,1616:3,1618:4,1617:5,1611:6,1612:7,1613:8, 95:9}\n",
    "\n",
    "def get_diacritic_hot_vector(haraka):\n",
    "    vector = [0 for _ in range(9)]\n",
    "    print(\"haraka:\" + haraka)\n",
    "    vector[harakat[ord(haraka)] - 1] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiacriticEncoding(wordDi):\n",
    "    word_embedding = list()\n",
    "    for w in wordDi:\n",
    "        if (len(w) > 1):\n",
    "            # print(ord(w[0]))\n",
    "            print(ord(w[1]))\n",
    "            w = w[0]\n",
    "        word_embedding.append(get_diacritic_hot_vector(w))\n",
    "    return word_embedding\n",
    "# getDiacriticEncoding('ًُ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:_\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:_\n",
      "1617\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:ٍ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ٌ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ٍ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:_\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ٌ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ٌ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:_\n",
      "1617\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:_\n",
      "1617\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:ِ\n",
      "haraka:ُ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ٍ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ً\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ٌ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:_\n",
      "haraka:_\n",
      "haraka:_\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ً\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ً\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ً\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:ِ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:_\n",
      "haraka:_\n",
      "1617\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "1617\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:ٌ\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ٍ\n",
      "haraka:ُ\n",
      "haraka:ْ\n",
      "haraka:ِ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ٌ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:ُ\n",
      "haraka:ُ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ً\n",
      "haraka:ِ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ْ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:ِ\n",
      "haraka:َ\n",
      "haraka:ً\n",
      "haraka:َ\n",
      "haraka:َ\n",
      "haraka:_\n",
      "haraka:َ\n",
      "haraka:ُ\n",
      "haraka:_\n",
      "haraka:ُ\n"
     ]
    }
   ],
   "source": [
    "X  = list() # input\n",
    "Y  = list() # output\n",
    "for i in range(len(embeddingsSentences)):\n",
    "    for j in range(len(embeddingsSentences[i])):\n",
    "        x = list()\n",
    "        x.append(embeddingsSentences[i])\n",
    "        x.append(getCharacterEncoding(sentences_without_diacritics[i][j]))\n",
    "        x.append(j)\n",
    "        X.append(x)\n",
    "        ### y ###\n",
    "        y = getDiacriticEncoding(diacritics[i][j])\n",
    "        Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([-8.6194072e-03,  3.6662782e-03,  5.1877904e-03,  5.7404060e-03,\n",
      "        7.4694911e-03, -6.1716656e-03,  1.1047125e-03,  6.0493494e-03,\n",
      "       -2.8413848e-03, -6.1743795e-03, -4.1114856e-04, -8.3720060e-03,\n",
      "       -5.6005633e-03,  7.1064476e-03,  3.3553925e-03,  7.2244219e-03,\n",
      "        6.7994562e-03,  7.5301109e-03, -3.7890249e-03, -5.6519831e-04,\n",
      "        2.3484014e-03, -4.5158374e-03,  8.3882548e-03, -9.8603358e-03,\n",
      "        6.7657670e-03,  2.9121095e-03, -4.9342490e-03,  4.3973173e-03,\n",
      "       -1.7399262e-03,  6.7108874e-03,  9.9654859e-03, -4.3622348e-03,\n",
      "       -5.9633976e-04, -5.6975926e-03,  3.8477452e-03,  2.7883479e-03,\n",
      "        6.8911030e-03,  6.1023729e-03,  9.5375814e-03,  9.2732897e-03,\n",
      "        7.9012951e-03, -6.9908272e-03, -9.1558043e-03, -3.5499129e-04,\n",
      "       -3.0983118e-03,  7.8949267e-03,  5.9358068e-03, -1.5467355e-03,\n",
      "        1.5131170e-03,  1.7902276e-03,  7.8178952e-03, -9.5113264e-03,\n",
      "       -2.0500859e-04,  3.4705850e-03, -9.3895441e-04,  8.3821947e-03,\n",
      "        9.0131443e-03,  6.5367981e-03, -7.1015640e-04,  7.7107474e-03,\n",
      "       -8.5336566e-03,  3.2074919e-03, -4.6373145e-03, -5.0883968e-03,\n",
      "        3.5898886e-03,  5.3711259e-03,  7.7697351e-03, -5.7664551e-03,\n",
      "        7.4295788e-03,  6.6275406e-03, -3.7124171e-03, -8.7439911e-03,\n",
      "        5.4386612e-03,  6.5102302e-03, -7.8452838e-04, -6.7104525e-03,\n",
      "       -7.0853899e-03, -2.4975927e-03,  5.1405411e-03, -3.6646074e-03,\n",
      "       -9.3700439e-03,  3.8266233e-03,  4.8804372e-03, -6.4263763e-03,\n",
      "        1.2076580e-03, -2.0741175e-03,  2.3799032e-05, -9.8820534e-03,\n",
      "        2.6960357e-03, -4.7455658e-03,  1.0892329e-03, -1.5740545e-03,\n",
      "        2.1967560e-03, -7.8800442e-03, -2.7152945e-03,  2.6649439e-03,\n",
      "        5.3472258e-03, -2.3897639e-03, -9.5090000e-03,  4.5050047e-03],\n",
      "      dtype=float32), array([ 1.3303380e-03,  6.5409895e-03,  9.9767642e-03,  9.0559609e-03,\n",
      "       -8.0107907e-03,  6.4783921e-03, -5.7169106e-03, -9.6188008e-04,\n",
      "        4.6632701e-04,  6.5740799e-03,  4.4665565e-03,  4.5898678e-03,\n",
      "        9.4903987e-03,  3.9382311e-04, -6.0267118e-03, -6.3397507e-03,\n",
      "        6.4395033e-03, -5.2521806e-03, -2.8441711e-03,  4.0491386e-03,\n",
      "       -2.2792739e-03, -6.0240924e-03, -2.3161718e-03,  1.1859618e-03,\n",
      "        2.1750571e-03,  6.0791052e-03, -5.2234880e-03,  3.0761694e-03,\n",
      "        7.2428002e-03,  2.1965529e-03,  5.4015578e-03, -4.8409416e-03,\n",
      "        6.1620832e-03, -7.6195388e-03,  3.4884014e-03, -9.3160244e-03,\n",
      "       -2.6141645e-03, -9.0663200e-03, -1.6022684e-03, -5.3656218e-03,\n",
      "       -3.9198943e-03,  1.1415388e-03,  2.7983072e-03, -1.5247940e-03,\n",
      "       -8.1662014e-03, -5.9245578e-03,  8.1131910e-04, -3.9562485e-03,\n",
      "       -9.4204014e-03, -7.7503570e-04,  6.6371323e-03,  5.9719640e-03,\n",
      "       -9.9128978e-03,  3.1336888e-03, -5.9901085e-03, -9.1732824e-03,\n",
      "        1.7695579e-04, -3.7344784e-04, -6.9697024e-03, -6.2807305e-03,\n",
      "       -2.4169756e-03,  7.0857424e-03, -7.5415652e-03,  7.7144727e-03,\n",
      "       -4.7289874e-04,  1.1058869e-03,  9.4864974e-03,  4.7352463e-03,\n",
      "       -3.5941249e-03,  3.7514840e-03,  3.5113713e-03,  6.3369228e-03,\n",
      "        8.4219952e-05, -4.4227866e-03,  1.3280398e-03, -5.4231388e-03,\n",
      "        1.4211284e-03,  4.9281684e-03,  5.1425127e-03,  9.1897408e-03,\n",
      "       -7.5266012e-03, -5.4054847e-03,  6.4510074e-03,  1.3632554e-03,\n",
      "       -6.6183433e-03,  8.7773928e-04,  2.6786812e-03, -2.5120613e-03,\n",
      "       -4.9413806e-03,  5.0241216e-03,  9.6392985e-03, -7.3638135e-03,\n",
      "       -1.2438127e-04, -2.5600293e-03, -6.3558351e-03, -1.3677027e-03,\n",
      "       -5.2470746e-03,  9.0663070e-03, -5.7848883e-03,  3.6888288e-03],\n",
      "      dtype=float32)], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 1]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'numpy.ndarray\\\\\\'>\"})\\', \"<class \\'int\\'>\", \\'(<class \\\\\\'list\\\\\\'> containing values of types {\\\\\\'(<class \\\\\\\\\\\\\\'list\\\\\\\\\\\\\\'> containing values of types {\"<class \\\\\\\\\\\\\\'int\\\\\\\\\\\\\\'>\"})\\\\\\'})\\'})'}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mo:\\DriveFiles\\CCE\\Fall_2024\\NLP\\project_NLP\\tensormodel.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/o%3A/DriveFiles/CCE/Fall_2024/NLP/project_NLP/tensormodel.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, Y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/o%3A/DriveFiles/CCE/Fall_2024/NLP/project_NLP/tensormodel.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(X)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/o%3A/DriveFiles/CCE/Fall_2024/NLP/project_NLP/tensormodel.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(\"yyyyyyyyy\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/o%3A/DriveFiles/CCE/Fall_2024/NLP/project_NLP/tensormodel.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print(Y)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\moga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\moga\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1082\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1079\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1082\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1083\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1084\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1085\u001b[0m         )\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'numpy.ndarray\\\\\\'>\"})\\', \"<class \\'int\\'>\", \\'(<class \\\\\\'list\\\\\\'> containing values of types {\\\\\\'(<class \\\\\\\\\\\\\\'list\\\\\\\\\\\\\\'> containing values of types {\"<class \\\\\\\\\\\\\\'int\\\\\\\\\\\\\\'>\"})\\\\\\'})\\'})'}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\\'(<class \\\\\\'list\\\\\\'> containing values of types {\"<class \\\\\\'int\\\\\\'>\"})\\'})'})"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)\n",
    "# print(X)\n",
    "# print(\"yyyyyyyyy\")\n",
    "# print(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
