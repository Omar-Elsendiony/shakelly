{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import tensorflow as tf\n",
    "# # from tensorflow.keras.models import Model\n",
    "# # from tensorflow.keras.layers import Input, LSTM, Bidirectional, SimpleRNNCell, RNN, Dense,Layer\n",
    "# class SelectHiddenState(Layer):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(SelectHiddenState, self).__init__(**kwargs)\n",
    "\n",
    "#     def call(self, lstm_output, scalar_input):\n",
    "#         # Ensure scalar_input is an integer for indexing\n",
    "#         timestep_index = tf.cast(tf.squeeze(scalar_input, axis=-1), tf.int32)\n",
    "#         # Gather the specific hidden state for each batch\n",
    "#         selected_state = tf.gather(lstm_output, timestep_index, batch_dims=1, axis=1)\n",
    "#         return selected_state\n",
    "\n",
    "# # Example usage with your model\n",
    "# # lstm_output is from your BiLSTM layer\n",
    "# # scalar_input is your additional input\n",
    "# max_sentence_length = 100  # Maximum length of sentence embeddings\n",
    "# embedding_dim = 100        # Dimension of sentence embeddings\n",
    "# max_char_length = 15       # Maximum length of a word in characters\n",
    "# char_vocab_size = 36       # Number of unique characters\n",
    "# num_diacritics = 15         # Number of possible diacritics for each character, including no diacritic\n",
    "\n",
    "# # Parameters\n",
    "# lstm_units = 32\n",
    "\n",
    "# # Character input\n",
    "# char_input = Input(shape=(max_char_length, char_vocab_size))\n",
    "\n",
    "# # Inputs\n",
    "# sentence_input = Input(shape=(max_sentence_length, embedding_dim))\n",
    "# scalar_input = Input(shape=(1,), name='scalar_input')\n",
    "\n",
    "# # BiLSTM layer with return_state\n",
    "# bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "# bi_lstm_output, forward_h, forward_c, backward_h, backward_c = bi_lstm(sentence_input)\n",
    "# print( forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n",
    "# #, forward_h, forward_c, backward_h, backward_c\n",
    "# # Average the forward and backward states (or choose another method to combine them)\n",
    "# select_state_layer = SelectHiddenState()\n",
    "# hidden_state_nth_timestep = select_state_layer(bi_lstm_output, scalar_input)\n",
    "\n",
    "# #hidden_state_nth_timestep = bi_lstm_output[:, scalar_input[1], :]\n",
    "# print(bi_lstm_output.shape)\n",
    "\n",
    "# # RNN layer with initial state from BiLSTM\n",
    "# rnn_cell = SimpleRNNCell(64)\n",
    "# rnn_layer = RNN(rnn_cell, return_sequences=True)\n",
    "# rnn_output = rnn_layer(char_input,initial_state=hidden_state_nth_timestep)\n",
    "\n",
    "# # Output layer\n",
    "# output_layer = Dense(num_diacritics, activation='softmax')(rnn_output)\n",
    "\n",
    "# # Build and compile the model\n",
    "# # Assuming sentence_input and scalar_input are defined as Input layers\n",
    "# model = Model(inputs=[sentence_input, char_input,scalar_input], outputs=output_layer)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \n",
    "# Y = \n",
    "from preprocessing import *\n",
    "\n",
    "trainSet = readFile('dataset/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_without_diacritics, diacritics = get_sentences(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_without_diacriticsTest, diacriticsTest = get_sentences(\"يُسَنُّ أَنْ يُصَانَ عَنْ رَائِحَةٍ كَرِيهَةٍ مِنْ بَصَلٍ وَثُومٍ وَكُرَّاتٍ )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences_without_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecmodel = makeWord2VecModel(sentences_without_diacritics)\n",
    "keys = word2vecmodel.wv.key_to_index\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingsSentences(sentences, word2vecmodel):\n",
    "    embeddingSentences = [] # list of all sentences\n",
    "    keys = word2vecmodel.wv.key_to_index\n",
    "    for s in sentences:\n",
    "        embeddingTemp = []  # list for one sentence\n",
    "        for w in s:\n",
    "            if w in keys:\n",
    "                embeddingTemp.append(word2vecmodel.wv[w])\n",
    "            ### unknown OOV till now\n",
    "        embeddingSentences.append(embeddingTemp)\n",
    "    return embeddingSentences\n",
    "embeddingsSentences = getEmbeddingsSentences(sentences_without_diacritics, word2vecmodel)\n",
    "print(embeddingsSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harakat   = load_binary('diacritic2id','./')\n",
    "for i in harakat:\n",
    "    print(i,harakat[i])\n",
    "#يُسَنُّ أَنْ يُصَانَ عَنْ رَائِحَةٍ كَرِيهَةٍ مِنْ بَصَلٍ وَثُومٍ وَكُرَّاتٍ( 3 / 297 )\n",
    "\n",
    "corpusDiacList = []\n",
    "for sentence in diacritics:\n",
    "    sentenceDiacList = []\n",
    "    for word in sentence:\n",
    "        #merge each list \n",
    "        oneDiacStr = ''\n",
    "        diacWordList = []\n",
    "        for diac in word:\n",
    "            if diac == '_':\n",
    "                oneDiacStr = ''\n",
    "            else:    \n",
    "                oneDiacStr = ''.join(diac)\n",
    "            #print(oneDiacStr,harakat[oneDiacStr]) \n",
    "            diacWordList.append(get_diacritic_hot_vector(oneDiacStr)) \n",
    "        sentenceDiacList.append(diacWordList)       \n",
    "    corpusDiacList.append(sentenceDiacList)\n",
    "print(corpusDiacList)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diacritic_hot_vector(haraka):\n",
    "    vector = [0 for _ in range(len(harakat))]\n",
    "    # print(\"haraka:\" + haraka)\n",
    "    vector[harakat[haraka] ] = 1\n",
    "    return vector\n",
    "\n",
    "print(get_diacritic_hot_vector('َ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDiacriticEncoding(wordDi):\n",
    "    word_embedding = list()\n",
    "    for w in wordDi:\n",
    "        if (len(w) > 1):\n",
    "            # print(ord(w[0]))\n",
    "            print(ord(w[1]))\n",
    "            w = w[0]\n",
    "        word_embedding.append(get_diacritic_hot_vector(w))\n",
    "    return word_embedding\n",
    "# getDiacriticEncoding('ًُ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X  = numpy.array([]) # input\n",
    "# Y  = numpy.array(corpusDiacList, dtype=object) # output\n",
    "\n",
    "# import numpy as np\n",
    "# for i in range(len(embeddingsSentences)):\n",
    "#     for j in range(len(embeddingsSentences[i])):\n",
    "#         x = numpy.array([])\n",
    "#         x=np.append(x,numpy.array([embeddingsSentences[i]]))\n",
    "#         x=np.append(x,[getCharacterEncoding(sentences_without_diacritics[i][j])])\n",
    "#         x=np.append(x,j)\n",
    "#         X=np.append(X,x)\n",
    "#         ### y ###\n",
    "#         # y = getDiacriticEncoding(diacritics[i][j])\n",
    "#         # Y=np.append(Y,y)\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# X_embeddings = []  # List to hold embedding arrays\n",
    "# X_scalars = []     # List to hold scalar values\n",
    "# Y  = corpusDiacList # output\n",
    "\n",
    "# for i in range(len(embeddingsSentences)):\n",
    "#     for j in range(len(embeddingsSentences[i])):\n",
    "#         # Add embeddings\n",
    "#         X_embeddings.append(np.array(embeddingsSentences[i]))\n",
    "\n",
    "#         # Add scalar values as a tuple or list\n",
    "#         char_encoding = getCharacterEncoding(sentences_without_diacritics[i][j])\n",
    "#         X_scalars.append([char_encoding, j])\n",
    "\n",
    "#         # For Y\n",
    "#         # y = getDiacriticEncoding(diacritics[i][j])\n",
    "#         # Y.append(y)\n",
    "\n",
    "# # Convert to tensors\n",
    "# X_embeddings_tensor = tf.ragged.constant(X_embeddings)  # Ragged tensor for embeddings\n",
    "# X_scalars_tensor =  tf.ragged.constant(X_scalars,shape=())      # Regular tensor for scalars\n",
    "# Y_tensor = tf.convert_to_tensor(Y)\n",
    "\n",
    "# # Check the shapes\n",
    "# print(X_embeddings_tensor.shape)\n",
    "# print(X_scalars_tensor.shape)\n",
    "# print(Y_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X  = list() # input\n",
    "# Y  = list() # output\n",
    "# for i in range(len(embeddingsSentences)):\n",
    "#     for j in range(len(embeddingsSentences[i])):\n",
    "#         x = list()\n",
    "#         x.append(embeddingsSentences[i])\n",
    "#         x.append(getCharacterEncoding(sentences_without_diacritics[i][j]))\n",
    "#         x.append(j)\n",
    "#         X.append(x)\n",
    "#         ### y ###\n",
    "#         y = getDiacriticEncoding(diacritics[i][j])\n",
    "#         Y.append(y)\n",
    "    \n",
    "# print(X[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# corpus=np.array(corpusDiacList, dtype=object).flatten()\n",
    "# print(corpus.shape)\n",
    "# corpus.flatten()\n",
    "# print(corpus.flatten()[2])\n",
    "#corpusDiacList=np.array(corpusDiacList)\n",
    "print(corpusDiacList)\n",
    "flattened = [item for sublist in corpusDiacList for item in sublist]\n",
    "Y__padded = pad_sequences(flattened, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "print(Y__padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_sentence = []  # List for sentence_input\n",
    "X_char = []      # List for char_input\n",
    "X_scalar = []    # List for scalar_input\n",
    "flattened = [item for sublist in corpusDiacList for item in sublist]\n",
    "Y__padded = pad_sequences(flattened, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "          # List for output\n",
    "\n",
    "for i in range(len(embeddingsSentences)):\n",
    "    for j in range(len(embeddingsSentences[i])):\n",
    "        # Add embeddings\n",
    "        X_sentence.append(embeddingsSentences[i])\n",
    "\n",
    "        # Prepare char_input\n",
    "        char_encoding = getCharacterEncoding(sentences_without_diacritics[i][j])\n",
    "        # Ensure char_encoding is shaped as (max_char_length, char_vocab_size)\n",
    "        # This might require reshaping or padding depending on your getCharacterEncoding function\n",
    "        X_char.append(char_encoding)\n",
    "\n",
    "        # Scalar input\n",
    "        X_scalar.append([j])\n",
    "\n",
    "        # Output\n",
    "        # y = getDiacriticEncoding(diacritics[i][j])\n",
    "        # Y.append(y)\n",
    "\n",
    "# Padding X_sentence\n",
    "# Assuming max_sentence_length and embedding_dim are defined\n",
    "X_sentence_padded = pad_sequences(X_sentence, maxlen=max_sentence_length, padding='post', dtype='float32')\n",
    "X_character_padded = pad_sequences(X_char, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "# Y__padded = pad_sequences(Y, maxlen=max_char_length, padding='post', dtype='float32')\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_sentence = np.array(X_sentence_padded)\n",
    "X_char = np.array(X_character_padded)\n",
    "X_scalar = np.array(X_scalar)\n",
    "\n",
    "Y = np.array(Y__padded)\n",
    "\n",
    "print(X_sentence.shape, X_char.shape, X_scalar.shape, Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectHiddenState(Layer):\n",
    "    def _init_(self, **kwargs):\n",
    "        super(SelectHiddenState, self)._init_(**kwargs)\n",
    "\n",
    "    def call(self, lstm_output, scalar_input):\n",
    "        timestep_index = tf.cast(tf.squeeze(scalar_input, axis=-1), tf.int32)\n",
    "        selected_state = tf.gather(lstm_output, timestep_index, batch_dims=1, axis=1)\n",
    "        return selected_state\n",
    "\n",
    "# Parameters\n",
    "max_sentence_length = 100\n",
    "embedding_dim = 100\n",
    "max_char_length = 15\n",
    "char_vocab_size = 36\n",
    "num_diacritics = 15\n",
    "lstm_units = 32\n",
    "\n",
    "# Inputs\n",
    "char_input = Input(shape=(max_char_length, char_vocab_size))\n",
    "sentence_input = Input(shape=(max_sentence_length, embedding_dim))\n",
    "scalar_input = Input(shape=(1,), name='scalar_input')\n",
    "\n",
    "# Padding layer for sentence_input (adjust padding as needed)\n",
    "# sentence_padding_layer = ZeroPadding1D(padding=(1, 1))  # Example padding\n",
    "# padded_sentence_input = sentence_padding_layer(sentence_input)\n",
    "# padding_layer = ZeroPadding1D(padding=(1, 1))  # Example padding\n",
    "# padded_char_input = padding_layer(char_input)\n",
    "# BiLSTM layer\n",
    "bi_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "bi_lstm_output, forward_h, forward_c, backward_h, backward_c = bi_lstm(sentence_input)\n",
    "print(forward_h)\n",
    "# Select state layer\n",
    "select_state_layer = SelectHiddenState()\n",
    "hidden_state_nth_timestep = select_state_layer(bi_lstm_output, scalar_input)\n",
    "\n",
    "# RNN layer\n",
    "rnn_cell = SimpleRNNCell(32)\n",
    "rnn_layer = RNN(rnn_cell, return_sequences=True)\n",
    "rnn_output = rnn_layer(char_input, initial_state=(forward_c+backward_c)/2)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(num_diacritics, activation='softmax')(rnn_output)\n",
    "\n",
    "# Model\n",
    "model = Model(inputs=[sentence_input, char_input, scalar_input], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit([X_sentence,X_char,X_scalar], Y, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
