{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 {'ِ', 'َ', 'ً', 'ْ', 'ٌ', 'ّ', 'ٍ', 'ُ'}\n",
      "ِ\n",
      "َ\n",
      "ً\n",
      "ْ\n",
      "ٌ\n",
      "ّ\n",
      "ٍ\n",
      "ُ\n",
      "36 {'ط', 'ف', 'خ', 'ر', 'س', 'م', 'و', 'ؤ', 'ء', 'ذ', 'أ', 'آ', 'ث', 'إ', 'د', 'ق', 'ا', 'ع', 'ب', 'ج', 'ل', 'ئ', 'ن', 'غ', 'ى', 'ي', 'ح', 'ة', 'ز', 'ص', 'ض', 'ت', 'ظ', 'ش', 'ه', 'ك'}\n",
      "َ 0\n",
      "ً 1\n",
      "ُ 2\n",
      "ٌ 3\n",
      "ِ 4\n",
      "ٍ 5\n",
      "ْ 6\n",
      "ّ 7\n",
      "َّ 8\n",
      "ًّ 9\n",
      "ُّ 10\n",
      "ٌّ 11\n",
      "ِّ 12\n",
      "ٍّ 13\n",
      " 14\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_binary(file, folder):\n",
    "    location  = os.path.join(folder, (file+'.pickle') )\n",
    "    with open(location, 'rb') as ff:\n",
    "        data = pickle.load(ff)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "diacritics = load_binary('diacritics','./')\n",
    "print(len(diacritics),diacritics)\n",
    "for k in diacritics:\n",
    "    print(k)\n",
    "\n",
    "letters = load_binary('arabic_letters','./')\n",
    "print(len(letters),letters)\n",
    "\n",
    "diacritic2id=load_binary('diacritic2id','./')\n",
    "for k,v in diacritic2id.items():\n",
    "    print(k,v)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len('اًً'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training set\n",
    "file_path = \"dataset/train.txt\"\n",
    "# Open the file for reading\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    # Read the content of the file\n",
    "    file_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data):\n",
    "    #return nltk.tokenize.sent_tokenize(data) #sent_tokenize(data.strip())\n",
    "    sentences = re.split(\"[\\n.،]+\", data)\n",
    "    # sentences = [s for sentence in senten]\n",
    "    words_per_sentence = []\n",
    "    tashkeel_per_sentence = []\n",
    "    for sentence in sentences:\n",
    "        if sentence == '': continue # new line may be followed by full stop\n",
    "        sentence = clear_punctuations(sentence)\n",
    "        # tashkel, length = get_tashkel(sentence)\n",
    "        # sentence = clear_tashkel(sentence)\n",
    "        sentence = word_tokenize(sentence.strip())\n",
    "        words_per_sentence.append(sentence)\n",
    "        # temp_tashkeel = []\n",
    "        # for word in sentence:\n",
    "        #     temp_tashkeel.append(get_tashkel(word))\n",
    "    \n",
    "    tashkeel_per_sentence = [[None for _ in w ]for w in words_per_sentence]\n",
    "    for i, x in enumerate(words_per_sentence):\n",
    "        for j, y in enumerate(x):\n",
    "            tashkeel = get_tashkel(y)\n",
    "            words_per_sentence[i][j] = clear_tashkel(y)\n",
    "            tashkeel_per_sentence[i][j] = tashkeel\n",
    "    # return [sent for sentence in sif line for sent in sent_tokenize(line.strip()) if sent]\n",
    "    #return [sent for line in data.split('\\n') if line for sent in sent_tokenize(line) if sent]\n",
    "    return words_per_sentence, tashkeel_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = clear_english_and_numbers(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_without_diacritics = clear_tashkel(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentence\n",
    "test_sentence = '''\n",
    "قَوْلُهُ : ( أَوْ قَطَعَ الْأَوَّلُ يَدَهُ إلَخْ ) قَالَ الزَّرْكَشِيُّ( 14 / 123 )\n",
    "ابْنُ عَرَفَةَ : قَوْلُهُ : بِلَفْظٍ يَقْتَضِيه كَإِنْكَارِ غَيْرِ حَدِيثٍ بِالْإِسْلَامِ وُجُوبَ مَا عُلِمَ وُجُوبُهُ مِنْ الدِّينِ ضَرُورَةً ( كَإِلْقَاءِ مُصْحَفٍ بِقَذَرٍ وَشَدِّ زُنَّارٍ ) ابْنُ عَرَفَةَ : قَوْلُ ابْنِ شَاسٍ : أَوْ بِفِعْلٍ يَتَضَمَّنُهُ هُوَ كَلُبْسِ الزُّنَّارِ وَإِلْقَاءِ الْمُصْحَفِ فِي صَرِيحِ النَّجَاسَةِ وَالسُّجُودِ لِلصَّنَمِ وَنَحْوِ ذَلِكَ ( وَسِحْرٍ ) مُحَمَّدٌ : قَوْلُ مَالِكٍ وَأَصْحَابِهِ أَنَّ السَّاحِرَ كَافِرٌ بِاَللَّهِ تَعَالَى قَالَ مَالِكٌ : هُوَ كَالزِّنْدِيقِ إذَا عَمِلَ السِّحْرَ بِنَفْسِهِ قُتِلَ وَلَمْ يُسْتَتَبْ .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_without_diacritic, diacritics = get_sentences(clear_english_and_numbers(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "قوله\n"
     ]
    }
   ],
   "source": [
    "test2 = 'قوله   أو قطع الأول يده إلخ  قال الزركشي'\n",
    "print(word_tokenize(test2.strip())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Define and train Word2Vec model\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=dataset_without_diacritic, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model (optional)\n",
    "word2vec_model.save(\"word2vec_model.model\")\n",
    "\n",
    "model_keys = word2vec_model.wv.key_to_index\n",
    "# print(word2vec_model.wv.vectors.shape)\n",
    "if 'وله' in model_keys: print(word2vec_model.wv['ق'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = 'قوله   او قطع الأول يده إلخ  قال الزركشي'\n",
    "\n",
    "get_char_vector(word_tokenize(test2.strip())[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "def getCharacterEncoding(word):\n",
    "    word_embedding = list()\n",
    "    for w in word:\n",
    "        word_embedding.append(get_char_vector(w))\n",
    "    return word_embedding\n",
    "\n",
    "getCharacterEncoding('ألزركش')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
